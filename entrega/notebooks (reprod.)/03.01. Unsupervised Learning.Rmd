
# 03.01 Unsuperivsed Learning

Unsupervised learning is a type of machine learning where the model learns patterns and relationships in the data without any explicit labels or target variables. It is used to discover hidden structures or patterns in the data.

In unsupervised learning, the model explores the data and identifies clusters, associations, or anomalies based on the inherent structure of the data. It can be useful for tasks such as data exploration, dimensionality reduction, and anomaly detection.

Some common algorithms used in unsupervised learning include clustering algorithms like k-means, hierarchical clustering, and DBSCAN, as well as dimensionality reduction techniques like principal component analysis (PCA) and t-SNE.

In this section, we will explore various unsupervised learning techniques and their applications.

## Setup

```{r}
library(keras)
library(plotly)
library(cluster)
library(factoextra)
library(Rtsne)
library(dbscan)
library(MASS)
use_python("/home/antonio/.virtualenvs/r-tensorflow/bin/python")

set.seed(33)

data <- read.csv("../data/raw/raw_data_train.csv", sep = ",", header = TRUE)
train_damage_grade <- data$damage_grade
data$building_id <- NULL
categorical_cols <- sapply(data, is.character)
data[categorical_cols] <-
  lapply(data[categorical_cols], function(x) as.numeric(factor(x)))
duplicated_data <- data[duplicated(data), ]
data <- data[!duplicated(data), ]
write.csv(data, "../data/processed/numeric_train.csv", row.names = FALSE)


test <- read.csv("../data/raw/raw_data_test.csv", sep = ",", header = TRUE)
test$building_id <- NULL
test[categorical_cols] <-
  lapply(test[categorical_cols], function(x) as.numeric(factor(x)))
duplicated_test <- test[duplicated(test), ]
test <- test[!duplicated(test), ]
write.csv(test, "../data/processed/numeric_test.csv", row.names = FALSE)


data_scaled <- scale(data)
test_scaled <- scale(test)

onehot_data <- read.csv(
  "../data/processed/unsupervised/onehot_train.csv",
  sep = ",",
  header = TRUE
)
onehot_damage_grade <- onehot_data$damage_grade
onehot_data$damage_grade <- NULL

onehot_test <- read.csv("../data/processed/unsupervised/onehot_test.csv",
  sep = ",",
  header = TRUE
)

pca_train <- read.csv("../data/processed/train/pca_train.csv",
  sep=",",
  header=TRUE
)
```

## Clustering

Clustering is a type of unsupervised learning where the goal is to group similar data points together based on their features. The data points within a cluster are more similar to each other than to data points in other clusters.

There are many techniques that can be used for clustering, such as k-means, hierarchical clustering, and OPTICS. In this section, we will explore k-means clustering and hierarchical clustering.

### K-Means Clustering

K-means clustering is a popular clustering algorithm that partitions the data into k clusters based on the distance between data points and the cluster centroids. The algorithm aims to minimize the sum of squared distances between data points and their assigned cluster centroids.

```{r}
# elbow method
fviz_nbclust(data_scaled, kmeans, method = "wss", k.max = 10) +
  labs(subtitle = "Elbow method")
# doesn't converge

# silhouette method
fviz_nbclust(data_scaled, kmeans, method = "silhouette", k.max = 25) +
  labs(subtitle = "Silhouette method")
# max silhouette values at 2 and 4 clusters
```

```{r}
# K-Means Clustering with 2 clusters
km_puntos <- kmeans(data_scaled, centers = 2)
km_puntos

cluster1_data <- data_scaled[km_puntos$cluster == 1, ]
cluster2_data <- data_scaled[km_puntos$cluster == 2, ]

# Create parallel coordinate plot
parcoord(cluster1_data, col = "red")
parcoord(cluster2_data, col = "blue")
```

### Hierarchical Clustering

```{r}
hc_puntos <- diana(test %>% scale())
hc_puntos$dc
fviz_dend(hc_puntos, k = 2, rect = TRUE)
```

```{r}
hc_puntos_ward <- hclust(dist(test %>% scale()), method = "ward.D2")
fviz_dend(hc_puntos_ward, k = 2, rect = TRUE)
```

## Dimensionality Reduction

### Autoencoders

Let's start by reducing the dimensionality of the data using autoencoders. Autoencoders are a type of neural network that learns to encode and decode the input data, effectively learning a compressed representation of the data.

```{r}
# define the encoder
encoder <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = ncol(onehot_data)) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu")

# define the decoder
decoder <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = 8) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = ncol(onehot_data), activation = "sigmoid")

# combine the encoder and decoder to create the autoencoder
autoencoder <- keras_model(inputs = encoder$input, outputs = decoder(encoder$output))

# compile the autoencoder
autoencoder %>% compile(optimizer = "adam", loss = "binary_crossentropy", metrics = c("mse", "mae"))

# prepare the data
to_normalize_cols <- c("count_floors_pre_eq", "age", "area_percentage", "height_percentage")

for (col in to_normalize_cols) {
  max_val <- max(c(max(onehot_data[[col]], na.rm = TRUE), max(onehot_test[[col]], na.rm = TRUE)))
  min_val <- min(c(min(onehot_data[[col]], na.rm = TRUE), min(onehot_test[[col]], na.rm = TRUE)))

  onehot_data[[col]] <- (onehot_data[[col]] - min_val) / (max_val - min_val)
  onehot_test[[col]] <- (onehot_test[[col]] - min_val) / (max_val - min_val)
}

x_data <- as.matrix(onehot_data)
x_train_index <- sample(1:nrow(onehot_data), 0.8 * nrow(onehot_data))
x_train <- onehot_data[x_train_index, ]
x_validation <- onehot_data[-x_train_index, ]
x_test <- as.matrix(onehot_test)
x_data <- array_reshape(x_data, c(nrow(x_data), ncol(onehot_data)))
x_train <- array_reshape(x_train, c(nrow(x_train), ncol(onehot_data)))
x_validation <- array_reshape(x_validation, c(nrow(x_validation), ncol(onehot_data)))
x_test <- array_reshape(x_test, c(nrow(x_test), ncol(onehot_test)))

# train the autoencoder
history <- autoencoder %>% fit(x_train, x_train, epochs = 150, batch_size = 256, validation_data = list(x_validation, x_validation))
plot(history)

autoencoder %>% evaluate(x_test, x_test, batch_size = 1)
```

The autoencoder compresses the input data into an 8-dimensional latent space. The results show that the autoencoder has learned a good representation of the data, with a binary crossentropy of 0.129, a mean squared error of 0.026 and a mean average error of 0.05 on the test set.

## Dimensionality Reduction in 3 dimensions

### Autoencoders

```{r}
# define the encoder
encoder <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = ncol(onehot_data)) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 3, activation = "relu")

# define the decoder
decoder <- keras_model_sequential() %>%
  layer_dense(units = 8, activation = "relu", input_shape = 3) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dense(units = ncol(onehot_data), activation = "sigmoid")

# combine the encoder and decoder to create the autoencoder
autoencoder <- keras_model(inputs = encoder$input, outputs = decoder(encoder$output))

# compile the autoencoder
autoencoder %>% compile(optimizer = "adam", loss = "binary_crossentropy", metrics = c("mse", "mae"))

# prepare the data
to_normalize_cols <- c("count_floors_pre_eq", "age", "area_percentage", "height_percentage")

for (col in to_normalize_cols) {
  max_val <- max(c(max(onehot_data[[col]], na.rm = TRUE), max(onehot_test[[col]], na.rm = TRUE)))
  min_val <- min(c(min(onehot_data[[col]], na.rm = TRUE), min(onehot_test[[col]], na.rm = TRUE)))

  onehot_data[[col]] <- (onehot_data[[col]] - min_val) / (max_val - min_val)
  onehot_test[[col]] <- (onehot_test[[col]] - min_val) / (max_val - min_val)
}

x_data <- as.matrix(onehot_data)
x_train_index <- sample(1:nrow(onehot_data), 0.8 * nrow(onehot_data))
x_train <- onehot_data[x_train_index, ]
x_validation <- onehot_data[-x_train_index, ]
x_test <- as.matrix(onehot_test)
x_data <- array_reshape(x_data, c(nrow(x_data), ncol(onehot_data)))
x_train <- array_reshape(x_train, c(nrow(x_train), ncol(onehot_data)))
x_validation <- array_reshape(x_validation, c(nrow(x_validation), ncol(onehot_data)))
x_test <- array_reshape(x_test, c(nrow(x_test), ncol(onehot_test)))

# train the autoencoder
history <- autoencoder %>% fit(x_train, x_train, epochs = 150, batch_size = 256, validation_data = list(x_validation, x_validation))
plot(history)

# evaluate the autoencoder
autoencoder %>% evaluate(x_test, x_test, batch_size = 1)
```

```{r}
# encode the data
encoded_data <- encoder %>% predict(x_data)
encoded_data <- as.data.frame(encoded_data)
encoded_data$damage_grade <- onehot_damage_grade

# save autoencoder model
save_model_hdf5(autoencoder, "../models/autoencoder_3d.h5")


# plot the encoded data in 3 dimensions
plot_ly(encoded_data, x = ~V1, y = ~V2, z = ~V3, color = ~damage_grade, colors = c("red", "green", "blue"), type = "scatter3d")
```

### t-SNE analysis

```{r}
tsne_data <- Rtsne(data, dims = 3, perplexity = 30, verbose = TRUE, max_iter = 500)
#plot in 3d with damage grade as color
plot_ly(x = tsne_data$Y[, 1], y = tsne_data$Y[, 2], z = tsne_data$Y[, 3], color = as.factor(data$damage_grade), colors = c("red", "green", "blue"), type = "scatter3d", mode = "markers")
```

### PCA

Although PCA has already been applied, we can try to visualize the first three principal components to try and extract any patterns.

```{r}
pca_three_dim <- pca_train[, 1:3]
pca_three_dim$damage_grade <- pca_train$damage_grade

plot_ly(pca_three_dim, x = ~PC1, y = ~PC2, z = ~PC3, color = ~damage_grade, colors = c("red", "green", "blue"), type = "scatter3d")
```

## Clustering in 3 dimensions

### K-Means Clustering

#### Autoencoder

```{r}
# elbow method
fviz_nbclust(encoded_data, kmeans, method = "wss", k.max = 10) +
  geom_vline(xintercept = 2, linetype = 2) +
  labs(subtitle = "Elbow method")
# elbow in k = 2

# silhouette method
fviz_nbclust(encoded_data, kmeans, method = "silhouette", k.max = 25) +
  geom_vline(xintercept = 2, linetype = 2) +
  labs(subtitle = "Silhouette method")
# max silhouette values at 2 clusters
```

```{r}
# K-Means Clustering with 2 clusters
km_puntos_3d <- kmeans(encoded_data[, 1:3], centers = 2)
km_puntos_3d

plot_ly(encoded_data, x = ~V1, y = ~V2, z = ~V3, color = as.factor(km_puntos_3d$cluster), colors = c("red", "blue"), type = "scatter3d")
```

#### t-SNE

```{r}
# elbow method
fviz_nbclust(tsne_data$Y, kmeans, method = "wss", k.max = 15) +
  labs(subtitle = "Elbow method")
# elbow doesn't converge

# silhouette method
fviz_nbclust(tsne_data$Y, kmeans, method = "silhouette", k.max = 15) +
  labs(subtitle = "Silhouette method")
# max silhouette values at 11 clusters
```

```{r}
# K-Means Clustering with 14 clusters
km_puntos_tsne <- kmeans(tsne_data$Y, centers = 11)
km_puntos_tsne

plot_ly(x = tsne_data$Y[, 1], y = tsne_data$Y[, 2], z = tsne_data$Y[, 3], color = as.factor(km_puntos_tsne$cluster), type = "scatter3d")
```

#### PCA

Although PCA has already been applied, we can try to visualize the first three principal components to try and extract any patterns.

```{r}
pca_three_dim <- pca_train[, 1:3]
pca_three_dim$damage_grade <- pca_train$damage_grade

plot_ly(pca_three_dim, x = ~PC1, y = ~PC2, z = ~PC3, color = ~damage_grade, colors = c("red", "green", "blue"), type = "scatter3d")
```

```{r}
# elbow method
fviz_nbclust(pca_train, kmeans, method = "wss", k.max = 10) +
  labs(subtitle = "Elbow method")
# doesn't converge

# silhouette method
fviz_nbclust(pca_train, kmeans, method = "silhouette", k.max = 25) +
  labs(subtitle = "Silhouette method")
# max silhouette values at 2 clusters
```

```{r}
km_pca <- kmeans(pca_train, centers = 2)
km_pca

plot_ly(pca_train, x = ~PC1, y = ~PC2, z = ~PC3, color = as.factor(km_pca$cluster), colors = c("red", "blue"), type = "scatter3d")
```

It seems as the best number of dimensions 2. This looks like it does not give any extra information to what we already know. So we won't be going further with this.

### DBSCAN

#### Autoencoder

```{r}
# determine the optimal eps value
kNNdistplot(encoded_data[, 1:3], k = 10)
abline(h = 1, col = "red", lty = 2)

dbscan_encoded <- dbscan(encoded_data[, 1:3], eps = 1, minPts = 6, )

plot_ly(encoded_data, x = ~V1, y = ~V2, z = ~V3, color = as.factor(dbscan_encoded$cluster), type = "scatter3d")
```

#### t-SNE

```{r}
# determine the optimal eps value
kNNdistplot(tsne_data$Y, k = 10)
abline(h = 3, col = "red", lty = 3)

dbscan_tsne <- dbscan(tsne_data$Y, eps = 3, minPts = 10)

plot_ly(x = tsne_data$Y[, 1], y = tsne_data$Y[, 2], z = tsne_data$Y[, 3], color = as.factor(dbscan_tsne$cluster), type = "scatter3d")
```

### Conclusions

From the analysis conducted, several important conclusions can be drawn: - **Clustering Insights**: The clustering analysis based on the original data reveals two primary clusters, with no clear patterns identified. - **Dimensionality Reduction Findings**: Visualization using dimensionality reduction techniques, such as PCA and t-SNE, highlights the complexity of the data. It is evident that the data is not linearly separable, which explains the suboptimal performance of simpler clustering algorithms. - **Autoencoders and PCA**: After applying Autoencoders and PCA, the two clusters identified in the preliminary clustering analysis persist. These clusters largely separate damage grade 1 from damage grades 2 and 3, offering a more nuanced view of the relationship between building characteristics and damage severity. However, the data suggests that it cannot effectively distinguish between damage grades 2 and 3 based on the available features, indicating a potential limitation. - **t-SNE Analysis**: t-SNE further complicates the scenario, revealing 13 distinct clusters, but with no clear correlation to damage grades.
