---
output: html_document
editor_options: 
  chunk_output_type: console
---
# 03.02. Supervised Learning

In this notebook, we will apply different classification algorithms to the different datasets we have obtained during preprocessing. The main goal is to find the best model for each dataset.

We will use the following datasets:
  - `onehot_train.csv`
  - `filtered_onehot_train.csv`
  - `pca_train.csv`
  - `smote_train.csv`
  - `weighted_train.csv`

We will using some test datasets to evaluate the models:
  - `onehot_test.csv`: this will be used for the models generated with `onehot_train.csv`.
  - `filtered_onehot_test.csv`: this will be used for the models generated with `filtered_onehot_train.csv`, `smote_train.csv` and `weighted_train.csv`.
  - `pca_test.csv`: this will be used for the models generated with `pca_train.csv`.

We will use the following classification algorithms:
 - Random Forest (ranger).
 - Gradient Boosting Machines (xgbTree).
 - Stochastic Gradient Boosting (gbm).
 
We will use the following metrics to evaluate the models:
 - Confusion Matrix.
 - ROC and AUC Multiclass Curves.
 
## Functions

We will define some functions to help us evaluate the models.

First, we will load the libraries we will use.

```{r}
library(tidyverse)
library(caret)
library(pROC)
library(ggplot2)
library(xgboost)
```

The following function will be used to apply the classification algorithms to the datasets. It will train the model, save it, predict the test data, save the predictions, and return the trained model.

```{r}
apply_algorithm <- function(algorithm, tuneGrid, train_data, test_data, model_path, predictions_path) {
  # Record start time
  start_time <- Sys.time()
  
  # Convert the target column to a factor
  train_data$damage_grade <- as.factor(train_data$damage_grade)
  levels(train_data$damage_grade) <- make.names(levels(train_data$damage_grade))
  
  # Train the model
  set.seed(123)
  model <- train(
    x = train_data %>% select(-damage_grade),
    y = train_data$damage_grade,
    method = algorithm,
    trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE, classProbs = TRUE),
    tuneGrid = tuneGrid
  )
  
  # Save the model
  save(model, file = model_path)
  
  # Prepare the test data
  test_data$damage_grade <- as.factor(test_data$damage_grade)
  levels(test_data$damage_grade) <- levels(train_data$damage_grade) # Ensure levels match training data
  
  # Predict the test data
  test_data$damage_grade_pred <- predict(model, newdata = test_data)
  
  # Predict with probabilities
  test_data$probabilities <- predict(model, newdata = test_data, type = "prob")
  
  # Save the predictions
  write.csv(test_data, predictions_path, row.names = FALSE)
  
  # Record end time
  end_time <- Sys.time()
  
  # Calculate total execution time
  total_time <- as.numeric(difftime(end_time, start_time, units = "secs"))
  minutes <- floor(total_time / 60)
  seconds <- round(total_time %% 60)
  cat("Total execution time:", minutes, "minute(s) and", seconds, "second(s).\n")
  
  # Return the trained model
  return(model)
}
```

The following function will be used to calculate the confusion matrix and other metrics.

The confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.

```{r}
get_prediction_metrics <- function(original_data, predicted_data) {
  # Convert the 'damage_grade' column to a factor
  original_data$damage_grade <- factor(original_data$damage_grade, levels = c("1", "2", "3"))
  
  # Convert the 'damage_grade_pred' column to a factor
  predicted_data$damage_grade_pred <- factor(predicted_data$damage_grade_pred, levels = c("X1", "X2", "X3"), labels = c("1", "2", "3"))
  
  # Calculate confusion matrix
  confusion <- confusionMatrix(data = predicted_data$damage_grade_pred, reference = original_data$damage_grade, mode = "everything")
  
  # Get other metrics in a table
  metrics_table <- as.data.frame(confusion$byClass)
  
  # Return the confusion matrix and metrics table
  return(list(confusion = confusion, metrics_table = metrics_table))
}
```

The following function will be used to calculate the ROC and AUC Multiclass Curves.

The ROC curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.

The AUC is the area under the ROC curve. AUC provides an aggregate measure of performance across all possible classification thresholds.

```{r}
get_AUC_multiclass <- function(original_data, predicted_data) {
  # Convert the 'damage_grade' column to a factor
  original_data$damage_grade <- factor(original_data$damage_grade, levels = c("1", "2", "3"))
  
  # Convert the 'damage_grade_pred' column to a factor
  predicted_data$damage_grade_pred <- factor(predicted_data$damage_grade_pred, levels = c("X1", "X2", "X3"), labels = c("1", "2", "3"))
  
  # Create the ROC curve
  multiclass_roc <- multiclass.roc(as.numeric(predicted_data$damage_grade_pred), as.numeric(original_data$damage_grade))
  
  # Print the AUC
  print(paste("AUC :", round(multiclass_roc$auc, 4)))
  
  # Return the AUC
  return(multiclass_roc)
}
```

## Onehot Dataset

We will load the `onehot_train.csv` and `onehot_test.csv` datasets and apply the classification algorithms to them.

```{r}
# Load the dataset
onehot_train <- read.csv("../data/processed/train/onehot_train.csv")

# Load the test dataset
onehot_test <- read.csv("../data/processed/test/onehot_test.csv")
```

### Random Forest

This method is a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. Ranger is a fast implementation of Random Forest for high-dimensional data.

Tuning parameters:
- `mtry` (#Randomly Selected Predictors)
- `splitrule` (Splitting Rule)
- `min.node.size` (Minimal Node Size)

```{r}
# Define the tuning grid
rf_onehot_tuneGrid <- expand.grid(.mtry = c(2, 3, 4, 5, 6, 7, 8, 9, 10),
                        .splitrule = "gini",
                        .min.node.size = c(1, 5, 10, 15, 20))

# Define the model and predictions paths
rf_onehot_model_path <- "../models/rf_onehot_model.RData"
rf_onehot_predictions_path <- "../data/predictions/rf_onehot_predictions.csv"

# Apply the algorithm
rf_onehot_model <- apply_algorithm("ranger", rf_onehot_tuneGrid, onehot_train, onehot_test, rf_onehot_model_path, rf_onehot_predictions_path)
```

Now we can evaluate the model using the confusion matrix and other metrics.

```{r}
# Load the predictions
rf_onehot_predictions <- read.csv(rf_onehot_predictions_path)

# Get the metrics
rf_onehot_metrics <- get_prediction_metrics(onehot_test, rf_onehot_predictions)
rf_onehot_metrics$confusion
rf_onehot_metrics$metrics_table
```

Finally, we can calculate the AUC.

```{r}
# Get the AUC
rf_onehot_auc <- get_AUC_multiclass(onehot_test, rf_onehot_predictions)
```

### Gradient Boosting Machines

Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.

Tuning parameters:

- `nrounds` (# Boosting Iterations)
- `max_depth` (Max Tree Depth)
- `eta` (Shrinkage)
- `gamma` (Minimum Loss Reduction)
- `colsample_bytree` (Subsample Ratio of Columns)
- `min_child_weight` (Minimum Sum of Instance Weight)
- `subsample` (Subsample Percentage)

```{r}
# Define the tuning grid
gbm_onehot_tuneGrid <- expand.grid(
  .nrounds = c(50, 100),              
  .max_depth = c(3, 4),                
  .eta = c(0.1, 0.3),                  
  .gamma = c(0, 0.1),                  
  .colsample_bytree = c(0.7, 0.9),     
  .min_child_weight = c(1, 3),         
  .subsample = c(0.7, 0.9)             
)

# Define the model and predictions paths
gbm_onehot_model_path <- "../models/gbm_onehot_model.RData"
gbm_onehot_predictions_path <- "../data/predictions/gbm_onehot_predictions.csv"

# Apply the algorithm
gbm_onehot_model <- apply_algorithm("xgbTree", gbm_onehot_tuneGrid, onehot_train, onehot_test, gbm_onehot_model_path, gbm_onehot_predictions_path)
```

Now we can evaluate the model using the confusion matrix and other metrics.

```{r}
# Load the predictions
gbm_onehot_predictions <- read.csv(gbm_onehot_predictions_path)

# Get the metrics
gbm_onehot_metrics <- get_prediction_metrics(onehot_test, gbm_onehot_predictions)
gbm_onehot_metrics$confusion
gbm_onehot_metrics$metrics_table
```

Finally, we can calculate the AUC.

```{r}
# Get the AUC
gbm_onehot_auc <- get_AUC_multiclass(onehot_test, gbm_onehot_predictions)
```

### Stochastic Gradient Boosting

Stochastic Gradient Boosting (SGB) is a generalization of boosting to arbitrary differentiable loss functions. It is particularly useful for classification problems.

Tuning parameters:

- `n.trees` (# Boosting Iterations)
- `interaction.depth` (Max Tree Depth)
- `shrinkage` (Shrinkage)
- `n.minobsinnode` (Min. Terminal Node Size)

```{r}
# Define the tuning grid
sgb_onehot_tuneGrid <- expand.grid(
  .n.trees = c(50, 100),              
  .interaction.depth = c(3, 4),       
  .shrinkage = c(0.1, 0.3),           
  .n.minobsinnode = c(10, 20)         
)

# Define the model and predictions paths
sgb_onehot_model_path <- "../models/sgb_onehot_model.RData"
sgb_onehot_predictions_path <- "../data/predictions/sgb_onehot_predictions.csv"

# Apply the algorithm
sgb_onehot_model <- apply_algorithm("gbm", sgb_onehot_tuneGrid, onehot_train, onehot_test, sgb_onehot_model_path, sgb_onehot_predictions_path)
```

Now we can evaluate the model using the confusion matrix and other metrics.

```{r}
# Load the predictions
sgb_onehot_predictions <- read.csv(sgb_onehot_predictions_path)

# Get the metrics
sgb_onehot_metrics <- get_prediction_metrics(onehot_test, sgb_onehot_predictions)
sgb_onehot_metrics$confusion
sgb_onehot_metrics$metrics_table
```

Finally, we can calculate the AUC.

```{r}
# Get the AUC
sgb_onehot_auc <- get_AUC_multiclass(onehot_test, sgb_onehot_predictions)
```

## Filtered Onehot Dataset

We will load the `filtered_onehot_train.csv` and `filtered_onehot_test.csv` datasets and apply the classification algorithms to them.

```{r}
# Load the dataset
filtered_onehot_train <- read.csv("../data/processed/train/filtered_onehot_train.csv")

# Load the test dataset
filtered_onehot_test <- read.csv("../data/processed/test/filtered_onehot_test.csv")
```

### Random Forest

### Gradient Boosting Machines

### Stochastic Gradient Boosting

## PCA Dataset
 
## SMOTE Dataset
 
## Weighted Dataset


# F1-Score
```{r}
# Convert F1 values to numeric
x1 <- as.numeric(confusion_matrix_sgb_table$F1[1])
x2 <- as.numeric(confusion_matrix_sgb_table$F1[2])
x3 <- as.numeric(confusion_matrix_sgb_table$F1[3])


f1_mean <- mean(c(x1, x2, x3))
````