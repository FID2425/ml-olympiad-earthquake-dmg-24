---
output:
  pdf_document: default
  html_document: default
---

# Title

*Should be brief and explanatory* (0.25 points)

Why do buildings collapse? A predictive and descriptive analysis of earthquake damage

# Summary

*A paragraph summarizing the presented content* (0.5 points)

# Introduction

*Outline the general problem and where the data comes from* (0.5 points)

# Methodology

## Data

*Initial exploration of the data to understand the problem: preprocessing and visualization*

### Exploratory Analysis

The exploratory data analysis (EDA) phase was crucial for understanding the dataset, identifying patterns, and gaining insights into the predictive task. This phase involved examining the dataset's structure, distribution, and relationships between variables to inform subsequent preprocessing and modeling steps. The EDA process was divided into several key components:

#### 1. Data Overview

After loading the dataset, we conducted a preliminary examination to understand its structure and contents. The dataset consisted of 36 columns and 4000 rows, with each row representing a building's characteristics and damage grade. The target variable, `damage_grade`, was categorical and represented the severity of damage caused by an earthquake with an integer value in the range `[1,3]`. The dataset also contained several categorical and numerical features, each providing potentially valuable information for the predictive task.

#### 2. Data Consistency

Next step was to check the consistency of the data. We tried first to identify missing values and potential inconsistencies that could affect the quality of the predictive model. The dataset was found to be relatively clean, with no missing values in the columns.

#### 3. Data visualization

To gain deeper insights into the data, we decided to visualize the distribution of values for the different attributes available, as some of then, in particular the categorical were hard to interpret as their values were mostly given by single character. Such was the case of columns `position` or `surface_condition`.

We started by analyzing two features which represented percentages: `height_percentage` and `area_percentage`. These two appeared to be already normalized, as their values were in the range `[0,1]`. The distribution of these two features was visualized using boxplots, which showed that the majority of buildings had a height percentage of around 0.2 and an area of around 0.1. In terms of height there weren't many outliers, but in terms of area there were many buildings with values higher to that of the majority. However we decided to let those values as they were because it would make sense that in a city there would be buildings with a larger area than others. We did however infer that since the height of the buildings was mostly the same, that would mean that most buildings were either houses or small buildings.

Next we analyzed the other numerical features, which were `age`, `count_floors_pre_eq`, `count_families`. And as we suspected, most buildings were residential houses, as the majority, with a familiy or two at most, rarely more that that. We also found that most buildings were registered as being 0 years since construction, and there was also a spike at 30 years since construction, which could mean that wither there was a boom in construction 30 years ago, or that the data was not properly recorded.

When it comes to the categorical features, we found that there was a lot of redundant data, for example, `plan_configuration` or `legal_ownership_status` had a value distribution that was mostly the same, with one value being the majority, and the set of `secondary_use_X` also had a low value distribution, with most not having any secondary use, which does fit with our hypothesis of most buildings being residential houses. On the same line of categorical features we tried to extract any realtion or meaning behind `surface_condition` and `position`, but we couldn't find any, as the values were mostly single characters and thus not very explanatory and there did not seem to be much correlation at first sight.

#### 4. Data Correlation

We also conducted a linear correlation analysis to identify relationships between numerical features and the target variable, `damage_grade`, and found the following features presented the highest correlation values:
-   `age` (0.269): We can infer that the age of the building is a significant factor in the damage grade, which is expected as older buildings are more likely to be damaged.
-   `area_percentage` (-0.325): Larger area percentages are correlated with lower damage grades, which could mean that larger buildings might experience less relative damage.
-   `roof_type` (-0.324): We can infer that the roof type is a significant factor in the damage grade, which is expected as some roof types are more resistant to earthquakes.
-   `has_superstructure_mud_mortar_stone` (0.393), `has_superstructure_cement_mortar_brick` (-0.415), `has_superstructure_rc_non_engineered` (-0.221) and `has_superstructure_rc_engineered` (-0.259): We can infer that the superstructure material is a significant factor in the damage grade, which is expected as some materials are more resistant to earthquakes.

#### 5. Further Analysis

Before taking any conclusions on the state of the data, we decided to carry out an analysis using the `DataExplorer` library, which report can be found alongside this document. The report did not shed light on anything that we hadn't already seen, but it served to reassure that the exploratory analysis we had done was correct. Overall the data seemed to be quite consistent and clean, although containing close to no variation in some columns, which could be a problem for the predictive model. And within some of those columns we decided some were best to be removed or processed in an specific way to reduce data complexity and potentially improve the model.

#### 6. Proposed Preprocessing

After conducting the exploratory analysis, we proposed the following:

- Considering one hot encoding for the categorical features.
- Applying techniques to deal with the imbalanced data on the target class `damage_grade`.
- Removing seemingly redundant columns to reduce dimensionality and noise in the data.
- Potentially combining the `has_secondary_use` columns into a single column to simplify the data.

When it comes to removing columns, we propose a few that could removed if needed to reduce dimensionality or noise in the data, as we believe it wouldn't affect much the results. Taking in mind of course that building preliminary models to confirm this would be beneficial still:
- `building_id`
- `plan_configuration`
- `legal_ownership_status`
- `count_families`
- `has_secondary_use_use_police`
- `has_secondary_use_gov_office`
- `has_secondary_use_health_post`
- `has_secondary_use_industry`
- `has_secondary_use_institution`
- `has_secondary_use_school`
- `has_secondary_use_other`

### Preprocessing

After the exploratory data analysis (EDA), we decided on a structured preprocessing approach to ensure the dataset was prepared for effective machine learning modeling. This phase included several crucial steps designed to clean the data, handle imbalanced class distributions, and format the dataset for optimal performance during model training and evaluation. The preprocessing steps are outlined below:

#### 1. Data Cleaning

The first step involved cleaning the dataset to ensure data integrity and consistency. Unnecessary columns that did not contribute to the predictive task were removed. Missing values were addressed by employing appropriate imputation techniques or removing rows/columns where necessary. 
- **Column Unification:** The `has_secondary_use` column was recalculated using related columns (`has_secondary_use_etc`) to address potential inconsistencies. The new unified column ensures centralized and reliable information.
- **Column Elimination:** Based on insights from the EDA, the following columns were removed: `building_id`, `plan_configuration`, `legal_ownership_status`, `count_families`, `has_secondary_use_police`, `has_secondary_use_gov_office`, `has_secondary_use_health_post`, `has_secondary_use_industry`, `has_secondary_use_institution`, `has_secondary_use_school`, and `has_secondary_use_other`. These columns were deemed non-contributive to the predictive task. After these changes, the dataset was reduced to 26 columns. A CSV file was generated at this stage for future use.

#### 2. Train-Test Split

The dataset was split into training and testing subsets using a stratified approach. Stratification ensured that the distribution of the target variable remained consistent across both subsets, providing a robust basis for model evaluation. The `caret` library was used to implement this split, ensuring a clear and reproducible methodology.

#### 3. One-Hot Encoding

Categorical variables were transformed into a numerical format using one-hot encoding. This process involved creating binary columns for each category within variables like `land_surface_condition`, `foundation_type`, `roof_type`, `ground_floor_type`, `other_floor_type`, and `position`. This step significantly increased the number of columns, making the data compatible with machine learning algorithms.

#### 4. Correlation Analysis

To identify and handle multicollinearity, a correlation analysis was performed. Two pairs of highly correlated columns were identified: `land_surface_condition_n` and `position_s`, and `land_surface_condition_t` and `position_t`. The columns with the lowest variance in each pair were removed to retain the most informative features. This step helped reduce redundancy in the dataset and improve model interpretability. We saved this dataset as `filtered_onehot` for further analysis.

#### 5. Principal Component Analysis (PCA)

Principal Component Analysis (PCA) was applied to reduce the dimensionality of the dataset.The data was standardized to ensure equal scaling across all features. Then, we used the Kaiser criterion to retain the first 13 principal components, preserving significant variance while reducing the dataset’s complexity. The target variable (`damage_grade`) was re-added after applying PCA. We saved this dataset as `pca` for subsequent analysis.

#### 6. Handling Imbalanced Data

Class imbalance, a common challenge in machine learning, was addressed using two complementary techniques:

-   **SMOTE (Synthetic Minority Oversampling Technique):** To balance the dataset, synthetic samples were generated for the minority class. SMOTE uses the nearest neighbors of existing minority class instances to create new, similar samples. This technique increased the representation of the minority class, enabling the model to learn effectively from all classes. The implementation was carried out using the `smotefamily` package in R, ensuring practical applicability and reproducibility. We saved this dataset as `smote` for further analysis.

-   **Weight Allocation:** To further mitigate the effects of class imbalance, weights were assigned to samples based on their class distribution. Higher weights were given to minority class samples, encouraging the model to prioritize these instances during training. This approach reduced the risk of biased predictions and improved overall model performance. The weighted dataset was saved as `weighted` for subsequent analysis.

### Summary

The preprocessing phase established a strong foundation for machine learning by cleaning the data, addressing class imbalance, and ensuring that the dataset was properly formatted. Each step—from data cleaning to PCA—was implemented with the goal of enhancing model performance and ensuring robust and reliable predictions. By following these preprocessing steps, the dataset was effectively prepared for further analysis and model development.

## Analysis I

*Tasks: supervised analysis, unsupervised analysis, comparison of results, etc.* (3 points)

## Analysis II

*Results: tables and/or figures presenting the obtained results* (1.5 points)

## Discussion

*Conclusions that can be drawn from the results* (1.5 points)

# Conclusions

*A brief paragraph that "dialogues" with the initial summary* (0.5 points)

# Bibliography

*References to the data and, if relevant, additional sources* (0.5 points)

# Optional

*Deep dive into topics or techniques not covered in class (e.g., filters, wrappers, etc.)* (3 points)
