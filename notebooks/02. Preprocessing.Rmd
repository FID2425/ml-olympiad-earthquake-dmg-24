# 02. Preprocessing

## Setup

```{r}
# Load libraries
library(tidyverse)

# Load the data
data <- read.csv("../data/raw/train.csv")

# View data
head(data)
dim(data)
str(data)
```

## Column elimination
According to the conclusions of the Exploratory Analysis, we will eliminate the following columns:

- `building_id`
- `plan_configuration`
- `legal_ownership_status`
- `count_families`
- `has_secondary_use_use_police`
- `has_secondary_use_gov_office`
- `has_secondary_use_health_post`
- `has_secondary_use_industry`
- `has_secondary_use_institution`
- `has_secondary_use_school`
- `has_secondary_use_other`

```{r}
reduced_data <- data %>%
  select(-building_id, -plan_configuration, -legal_ownership_status, -count_families, -has_secondary_use_use_police, -has_secondary_use_gov_office, -has_secondary_use_health_post, -has_secondary_use_industry, -has_secondary_use_institution, -has_secondary_use_school, -has_secondary_use_other)

# View data
head(reduced_data)
dim(reduced_data)
str(reduced_data)
```

As we can see, after eliminating the mentioned columns, the data has 26 columns.

## Label encoding

In this document, we will be performing label encoding on character-type columns. Label encoding converts categorical data into numerical values, allowing machine learning algorithms to handle non-numeric data more effectively.

### Identify character-type columns

```{r}
# show only chr type columns
reduced_data %>% select_if(is.character) %>% colnames()
```

To ensure we understand the contents of the character-type columns, we will display the unique values within each column. Then, we will use the `mutate` function in combination with `across` to apply label encoding to the selected columns. We convert each category into a numerical representation using the `factor` function, which assigns integer codes to each unique value.

```{r}

# show unique values in each column
reduced_data %>% select_if(is.character) %>% map(unique)


encoded_df <- reduced_data %>%
  mutate(
    across(
      c(
        land_surface_condition,
        foundation_type,
        roof_type,
        ground_floor_type,
        other_floor_type,
        position
      ),
      ~ as.numeric(factor(.))
    )
  )
```

### Creating Equivalence Tables for Each Encoded Column

To understand the mapping between original and encoded values, we create a function that generates and prints equivalence tables for each column. This function outputs a table showing the original category and its corresponding numerical code.

```{r}
# Función para crear y mostrar una tabla de equivalencias por cada columna
 crear_tabla_equivalencias <- function(col) {
   tabla_equivalencias <- tibble(
     original = unique(reduced_data[[col]]),
     encoded = as.numeric(factor(unique(reduced_data[[col]])))
   )
   cat("\n### Tabla de equivalencias para:", col, "\n")
   print(tabla_equivalencias)
 }

# Generar una tabla por cada columna
 walk(
   c("land_surface_condition", "foundation_type", "roof_type", "ground_floor_type", "other_floor_type", "position"),
   crear_tabla_equivalencias
 )
```

#### land_surface_condition

| Original | Encoded |
|----------|---------|
| o        | 2       |
| t        | 3       |
| n        | 1       |

#### foundation_type

| Original | Encoded |
|----------|---------|
| r        | 3       |
| i        | 2       |
| u        | 4       |
| w        | 5       |
| h        | 1       |

#### roof_type

| Original | Encoded |
|----------|---------|
| x        | 3       |
| n        | 1       |
| q        | 2       |

#### ground_floor_type

| Original | Encoded |
|----------|---------|
| f        | 1       |
| v        | 3       |
| x        | 4       |
| m        | 2       |
| z        | 5       |

#### other_floor_type

| Original | Encoded |
|----------|---------|
| q        | 2       |
| s        | 3       |
| j        | 1       |
| x        | 4       |

#### position

| Original | Encoded |
|----------|---------|
| s        | 3       |
| t        | 4       |
| j        | 1       |
| o        | 2       |

## Column unification

### Unify Has_secondary_use columns

We have decided to remove the existing `has_secondary_use` column from our dataset. The main reason for this decision is the potential presence of inconsistent data, which could affect the quality and accuracy of future analyses.

To ensure the dataset's integrity, we have recalculated the `has_secondary_use` column using the related columns (`has_secondary_use_etc`). This decision helps us:

-   **Reduce noise** in the dataset by removing the multiple columns (`has_secondary_use_etc`) that previously complicated the analysis.
-   **Centralize information** into a single, consistent column (`has_secondary_use`) that has been verified and adjusted for reliability.

With this approach, we aim to make future analyses clearer and based on more trustworthy data.

```{r}
## Remove the has_secondary_use column
encoded_df <- encoded_df %>% select(-has_secondary_use)

unified_df <- encoded_df %>%
  mutate(has_secondary_use = if_else(
    rowSums(select(., starts_with("has_secondary_use"))) > 0, 
    1, 
    0
  )) %>%
  select(-starts_with("has_secondary_use_"))

# print(unified_df %>% select(has_secondary_use) %>% head(10))
# Número de muestras con has_secondary_use = 0
print(unified_df %>% filter(has_secondary_use == 0) %>% nrow())

# Número de muestras con has_secondary_use = 1
print(unified_df %>% filter(has_secondary_use == 1) %>% nrow())
```

Finally, we are going to store the processed data in a new variable, in order to use it in the next steps.
```{r}
processed_data <- unified_df
```


## Principal Component Analysis (PCA)

PCA is a dimensionality reduction technique that transforms the data into a lower-dimensional space. This technique is useful for reducing the number of features in the dataset while retaining the most important information. In this section, we will perform PCA on the dataset to reduce the number of features.

First of all, we will take a look at the data we have obtained after the previous steps.

```{r}
head(processed_data)
dim(processed_data)
str(processed_data)
summary(processed_data)
```

Before applying PCA, we need to prepare the data by doing the following:

- Remove the target variable (`damage_grade`) from the dataset. We will add it back after applying PCA.
- Standardize the data as PCA is sensitive to the scale of the variables. We will use the `scale` function to standardize the numerical variables so that they have a mean of 0 and a standard deviation of 1.

```{r}
# Remove the target variable
data_without_target <- processed_data %>% select(-damage_grade)

# Standardize the data
data_scaled <- scale(data_without_target)
```

Now, we can apply PCA to the standardized data. We will use the `prcomp` function to perform PCA and extract the principal components. We can also view the PCA results, including the rotation matrix, which shows the correlation between the original variables and the principal components.

```{r}
pca_result <- prcomp(data_scaled, center = TRUE, scale. = TRUE)

# View the PCA results
summary(pca_result)

# Get the rotation matrix
pca_result$rotation
```

Now, we must decide how many principal components to keep. There are several methods to determine the number of components to retain, such as the Kaiser criterion, scree plot, and cumulative explained variance. In this case, we will use the Kaiser criterion, which suggests retaining components with eigenvalues greater than 1.

```{r}
# Calculate the eigenvalues
eigenvalues <- pca_result$sdev^2

# View the eigenvalues
eigenvalues

# Select the components with eigenvalues greater than 1
num_components <- sum(eigenvalues > 1)

# Number of components to keep
num_components
```

According to the Kaiser criterion, we should keep the first 6 principal components. We can extract these components from the PCA results and add the target variable back to the dataset after applying PCA.

```{r}
# Convert the principal components to a data frame
pca_components_df <- as.data.frame(pca_result$x)

# Extract the first 6 principal components
pca_components <- pca_components_df[, 1:num_components]

# Add the target variable back to the dataset
pca_data <- cbind(pca_components, processed_data$damage_grade)

head(pca_data)
dim(pca_data)
str(pca_data)
summary(pca_data)
```

As a result, we have obtained a dataset with 6 principal components and the target variable `damage_grade` after applying PCA using the Kaiser criterion. This suposes a significant reduction in the number of features. We can use this dataset for further analysis and modeling in the next steps.

## Imbalanced data handling
