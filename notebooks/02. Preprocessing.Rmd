# 02. Preprocessing

Preprocessing is a crucial step in the data analysis pipeline, as it helps to clean, transform, and prepare the data for further analysis and modeling. In this notebook, we will perform various preprocessing tasks on the dataset to ensure its quality and suitability for machine learning algorithms.

## Setup

```{r}
# Load libraries
library(tidyverse)

# Load the data
data <- read.csv("../data/raw/train.csv")

# View data
head(data)
dim(data)
str(data)
```

## Column unification

### Unify Has_secondary_use columns

We have decided to remove the existing `has_secondary_use` column from our dataset. The main reason for this decision is the potential presence of inconsistent data, which could affect the quality and accuracy of future analyses.

To ensure the dataset's integrity, we have recalculated the `has_secondary_use` column using the related columns (`has_secondary_use_etc`). This decision helps us:

-   **Reduce noise** in the dataset by removing the multiple columns (`has_secondary_use_etc`) that previously complicated the analysis.
-   **Centralize information** into a single, consistent column (`has_secondary_use`) that has been verified and adjusted for reliability.

With this approach, we aim to make future analyses clearer and based on more trustworthy data.

```{r}

print("Before unification")
# has_secondary_use = 0 number of samples
print(data %>% filter(has_secondary_use == 0) %>% nrow())

# has_secondary_use = 1 number of samples
print(data %>% filter(has_secondary_use == 1) %>% nrow())

## Remove the has_secondary_use column
new_df <- data %>% select(-has_secondary_use)

# Unify the has_secondary_use columns
data <- new_df %>%
  mutate(has_secondary_use = if_else(
    rowSums(select(., starts_with("has_secondary_use_"))) > 0, 
    1, 
    0
  ))

print("After unification")

# has_secondary_use = 0 number of samples
print(data %>% filter(has_secondary_use == 0) %>% nrow())

# has_secondary_use = 1 number of samples
print(data %>% filter(has_secondary_use == 1) %>% nrow())
```

## Column elimination

According to the conclusions of the Exploratory Analysis, we will eliminate the following columns:

-   `building_id`
-   `plan_configuration`
-   `legal_ownership_status`
-   `count_families`
-   `has_secondary_use_use_police`
-   `has_secondary_use_gov_office`
-   `has_secondary_use_health_post`
-   `has_secondary_use_industry`
-   `has_secondary_use_institution`
-   `has_secondary_use_school`
-   `has_secondary_use_other`

```{r}
reduced_data <- data %>%
  select(-building_id, -plan_configuration, -legal_ownership_status, -count_families, -has_secondary_use_use_police, -has_secondary_use_gov_office, -has_secondary_use_health_post, -has_secondary_use_industry, -has_secondary_use_institution, -has_secondary_use_school, -has_secondary_use_other)

# View data
head(reduced_data)
dim(reduced_data)
str(reduced_data)
```

As we can see, after eliminating the mentioned columns, the data has 26 columns.

## Column encoding

### One-Hot Encoding

In this section, we will perform one-hot encoding on the categorical variables in the dataset. One-hot encoding is a technique used to convert categorical variables into a numerical format that can be used in machine learning algorithms. This process involves creating binary columns for each category in a categorical variable.

We will apply one-hot encoding to the following categorical variables: - `land_surface_condition` - `foundation_type` - `roof_type` - `ground_floor_type` - `other_floor_type` - `position`

```{r}
# Perform one-hot encoding
## Convert categorical variables to binary columns

df_one_hot <- reduced_data

# Function to rename columns with a separator and create binary columns
rename_columns <- function(column_names) {
  one_hot_data <- model.matrix(as.formula(paste("~", column_names, "- 1")), data = reduced_data)
  colnames(one_hot_data) <- paste(column_names, sub(column_names, "", colnames(one_hot_data)), sep = "_")
  return(one_hot_data)
}

# Apply the function to each variable
one_hot_roof_type <- rename_columns("roof_type")
one_hot_land_surface_condition <- rename_columns("land_surface_condition")
one_hot_foundation_type <- rename_columns("foundation_type")
one_hot_ground_floor_type <- rename_columns("ground_floor_type")
one_hot_other_floor_type <- rename_columns("other_floor_type")
one_hot_position <- rename_columns("position")

# Combine the original data frame with the renamed one-hot encoded columns

df_one_hot <- cbind(
  reduced_data,
  one_hot_land_surface_condition,
  one_hot_foundation_type,
  one_hot_roof_type,
  one_hot_ground_floor_type,
  one_hot_other_floor_type,
  one_hot_position
) %>% select(-one_of(c("land_surface_condition", "foundation_type", "roof_type", 
                   "ground_floor_type", "other_floor_type", "position")))

# Convert one-hot encoded columns to integers
df_one_hot[] <- lapply(df_one_hot, as.integer)

```

Let's take a look at the data after applying one-hot encoding.

```{r}
head(df_one_hot)
dim(df_one_hot)
str(df_one_hot)
summary(df_one_hot)
```

## Correlation analysis

As we can see, after applying one-hot encoding, the number of columns has increased significantly. This transformation is necessary to convert categorical variables into a numerical format that can be used in machine learning algorithms. However, it is essential to check for correlation between the newly created binary columns, because maybe some of them are highly correlated and can be eliminated to reduce redundancy.

```{r}
df_correlation <- df_one_hot

# Calculate the correlation matrix
cor_matrix <- cor(df_correlation)

# Define the threshold for high correlation
threshold <- 0.75

# Find pairs of highly correlated columns
high_corr_pairs <- which(abs(cor_matrix) > threshold, arr.ind = TRUE)

# Filter out the diagonal and duplicate pairs
high_corr_pairs <- high_corr_pairs[high_corr_pairs[,1] < high_corr_pairs[,2], ]

# Extract the column names of the highly correlated columns
correlated_columns <- apply(high_corr_pairs, 1, function(pair) {
  colnames(cor_matrix)[pair]
})

# See results
correlated_columns
```

We have identified two pairs of highly correlated columns: 
- `land_surface_condition_n` and `position_s`
- `land_surface_condition_t` and `position_t`

Now, we have to decide which column to remove from each pair. We will remove the column with the lowest variance, as it is likely to contain less information.

```{r}
# Pair 1: land_surface_condition_n and position_s
# Calculate the variances of the columns
variances <- apply(df_correlation[, c("land_surface_condition_n", "position_s")], 2, var)

# Sort the variances in descending order
variances_sorted <- sort(variances, decreasing = TRUE)

# View the variances
print(variances_sorted)

# land_surface_condition_n has a lower variance, so we will remove it
df_correlation <- df_correlation %>% select(-land_surface_condition_n)


# Pair 2: land_surface_condition_t and position_t
# Calculate the variances of the columns
variances <- apply(df_correlation[, c("land_surface_condition_t", "position_t")], 2, var)

# Sort the variances in descending order
variances_sorted <- sort(variances, decreasing = TRUE)

# View the variances
print(variances_sorted)

# position_t has a lower variance, so we will remove it
df_correlation <- df_correlation %>% select(-position_t)

```

Finally, we are going to store the processed data in a new variable, in order to use it in the next steps.

```{r}
processed_data <- df_correlation
```


## Principal Component Analysis (PCA)

PCA is a dimensionality reduction technique that transforms the data into a lower-dimensional space. This technique is useful for reducing the number of features in the dataset while retaining the most important information. In this section, we will perform PCA on the dataset to reduce the number of features.

First of all, we will take a look at the data we have obtained after the previous steps.

```{r}
head(processed_data)
dim(processed_data)
str(processed_data)
summary(processed_data)
```

Before applying PCA, we need to prepare the data by doing the following:

-   Remove the target variable (`damage_grade`) from the dataset. We will add it back after applying PCA.
-   Standardize the data as PCA is sensitive to the scale of the variables. We will use the `scale` function to standardize the numerical variables so that they have a mean of 0 and a standard deviation of 1.

```{r}
# Remove the target variable
data_without_target <- processed_data %>% select(-damage_grade)

# Standardize the data
data_scaled <- scale(data_without_target)
```

Now, we can apply PCA to the standardized data. We will use the `prcomp` function to perform PCA and extract the principal components. We can also view the PCA results, including the rotation matrix, which shows the correlation between the original variables and the principal components.

```{r}
pca_result <- prcomp(data_scaled, center = TRUE, scale. = TRUE)

# View the PCA results
summary(pca_result)

# Get the rotation matrix
pca_result$rotation
```

Now, we must decide how many principal components to keep. There are several methods to determine the number of components to retain, such as the Kaiser criterion, scree plot, and cumulative explained variance. In this case, we will use the Kaiser criterion, which suggests retaining components with eigenvalues greater than 1.

```{r}
# Calculate the eigenvalues
eigenvalues <- pca_result$sdev^2

# View the eigenvalues
eigenvalues

# Select the components with eigenvalues greater than 1
num_components <- sum(eigenvalues > 1)

# Number of components to keep
num_components
```

According to the Kaiser criterion, we should keep the first 14 principal components. We can extract these components from the PCA results and add the target variable back to the dataset after applying PCA.

```{r}
# Convert the principal components to a data frame
pca_components_df <- as.data.frame(pca_result$x)

# Extract the first 14 principal components
pca_components <- pca_components_df[, 1:num_components]

# Add the target variable back to the dataset
pca_data <- cbind(pca_components, processed_data$damage_grade)

head(pca_data)
dim(pca_data)
str(pca_data)
summary(pca_data)
```

As a result, we have obtained a dataset with 14 principal components and the target variable `damage_grade` after applying PCA using the Kaiser criterion. This suposes a significant reduction in the number of features. We can use this dataset for further analysis and modeling in the next steps.

## Imbalance data handling

When working with imbalanced datasets, especially in predictive modeling, it is essential to address class imbalance to improve model performance and prevent biased results. There are numerous techniques available for balancing samples, including methods that generate synthetic data or assign weights to existing data points.

### Generate synthetic data

For this study, we explored various approaches to balance the distribution of our target variable, `damage_grade`. Initially, our dataset displayed the following distribution:

| 1   | 2    | 3    |
|-----|------|------|
| 729 | 1968 | 1303 |

To mitigate the imbalance, we applied **SMOTE (Synthetic Minority Oversampling Technique)**. SMOTE is a well-known algorithm to fight this problem. The general idea of this method is to artificially generate new examples of the minority class using the nearest neighbors of these cases. Furthermore, the majority class examples are also under-sampled, leading to a more balanced dataset.

First, we will check the distribution of the target variable `damage_grade` to confirm the imbalance in the dataset. Then, we will apply SMOTE to balance the data.


As we can see, the dataset is imbalanced, with a higher number of buildings in damage grade 2 compared to the other grades. We will use the `smotefamily` package to apply SMOTE to balance the data.

```{r}
library(smotefamily)

# Use damage_grade as factor
processed_data$damage_grade <- as.factor(processed_data$damage_grade)

# Apply SMOTE
smote_result <- SMOTE(X = processed_data[, -which(names(processed_data) == "damage_grade")], 
                      target = processed_data$damage_grade, 
                      K = 5, 
                      dup_size = 2)

synthetic_data_smote <- smote_result$data

dim(synthetic_data_smote)

## Print the columns of the synthetic data as an array
table(synthetic_data_smote$class)

### Save the data as categorical
synthetic_data_smote$class <- as.factor(synthetic_data_smote$class)

```
| 1    | 2    | 3    |
| ---- | ---- | ---- |
| 2187 | 1968 | 1303 |

The SMOTE algorithm has effectively increased the number of samples in the minority class (damage grade 1) by generating synthetic data based on the nearest neighbors. As a result, the distribution becomes more equitable, enhancing the model’s ability to correctly classify instances across all damage grades. This approach, combining oversampling of the minority class and under-sampling of the majority class, maintains a better balance and helps reduce potential overfitting to the dominant class.

### Weight allocation

Another approach to handle imbalanced data is to assign weights to the samples based on their class distribution. By assigning higher weights to the minority class samples, we can emphasize their importance during model training and improve the model's ability to learn from these instances.

```{r}
# Calculate the frequency of each class
class_freq <- table(processed_data$damage_grade)

# Calculate the total number of samples
total_samples <- sum(class_freq)

# Calculate the inverse weights for each class (total_samples / class frequency)
class_weights <- total_samples / (length(class_freq) * class_freq)

# Assign the weights to the samples
processed_data$weights <- sapply(processed_data$damage_grade, function(x) class_weights[as.character(x)])

## Show the column damage_grade and weights for unique values
unique(processed_data[, c("damage_grade", "weights")])
```

Now it is possible to use the weights assigned to each sample during model training to give more importance to the minority class instances. This approach helps to balance the impact of different classes on the model's learning process and improve the overall performance on imbalanced datasets.
