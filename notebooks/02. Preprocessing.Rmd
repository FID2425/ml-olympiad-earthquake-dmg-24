# 02. Preprocessing

## Setup

```{r}
# Load libraries
library(tidyverse)

# Load the data
data <- read.csv("../data/raw/train.csv")

# View data
head(data)
dim(data)
str(data)
```
## Column unification

### Unify Has_secondary_use columns

We have decided to remove the existing `has_secondary_use` column from our dataset. The main reason for this decision is the potential presence of inconsistent data, which could affect the quality and accuracy of future analyses.

To ensure the dataset's integrity, we have recalculated the `has_secondary_use` column using the related columns (`has_secondary_use_etc`). This decision helps us:

-   **Reduce noise** in the dataset by removing the multiple columns (`has_secondary_use_etc`) that previously complicated the analysis.
-   **Centralize information** into a single, consistent column (`has_secondary_use`) that has been verified and adjusted for reliability.

With this approach, we aim to make future analyses clearer and based on more trustworthy data.

```{r}

print("Before unification")
# has_secondary_use = 0 number of samples
print(data %>% filter(has_secondary_use == 0) %>% nrow())

# has_secondary_use = 1 number of samples
print(data %>% filter(has_secondary_use == 1) %>% nrow())

## Remove the has_secondary_use column
new_df <- data %>% select(-has_secondary_use)

# Unify the has_secondary_use columns
data <- new_df %>%
  mutate(has_secondary_use = if_else(
    rowSums(select(., starts_with("has_secondary_use_"))) > 0, 
    1, 
    0
  ))

print("After unification")

# has_secondary_use = 0 number of samples
print(data %>% filter(has_secondary_use == 0) %>% nrow())

# has_secondary_use = 1 number of samples
print(data %>% filter(has_secondary_use == 1) %>% nrow())
```

## Column elimination

According to the conclusions of the Exploratory Analysis, we will eliminate the following columns:

-   `building_id`
-   `plan_configuration`
-   `legal_ownership_status`
-   `count_families`
-   `has_secondary_use_use_police`
-   `has_secondary_use_gov_office`
-   `has_secondary_use_health_post`
-   `has_secondary_use_industry`
-   `has_secondary_use_institution`
-   `has_secondary_use_school`
-   `has_secondary_use_other`

```{r}
reduced_data <- data %>%
  select(-building_id, -plan_configuration, -legal_ownership_status, -count_families, -has_secondary_use_use_police, -has_secondary_use_gov_office, -has_secondary_use_health_post, -has_secondary_use_industry, -has_secondary_use_institution, -has_secondary_use_school, -has_secondary_use_other)

# View data
head(reduced_data)
dim(reduced_data)
str(reduced_data)
```

As we can see, after eliminating the mentioned columns, the data has 26 columns.

## Column encoding
### One-Hot Encoding

In this section, we will perform one-hot encoding on the categorical variables in the dataset. One-hot encoding is a technique used to convert categorical variables into a numerical format that can be used in machine learning algorithms. This process involves creating binary columns for each category in a categorical variable.

We will apply one-hot encoding to the following categorical variables:
- `land_surface_condition`
- `foundation_type`
- `roof_type`
- `ground_floor_type`
- `other_floor_type`
- `position`

```{r}
# Perform one-hot encoding
## Convert categorical variables to binary columns

df_one_hot <- reduced_data

# Function to rename columns with a separator and create binary columns
rename_columns <- function(column_names) {
  one_hot_data <- model.matrix(as.formula(paste("~", column_names, "- 1")), data = reduced_data)
  colnames(one_hot_data) <- paste(column_names, sub(column_names, "", colnames(one_hot_data)), sep = "_")
  return(one_hot_data)
}

# Apply the function to each variable
one_hot_roof_type <- rename_columns("roof_type")
one_hot_land_surface_condition <- rename_columns("land_surface_condition")
one_hot_foundation_type <- rename_columns("foundation_type")
one_hot_ground_floor_type <- rename_columns("ground_floor_type")
one_hot_other_floor_type <- rename_columns("other_floor_type")
one_hot_position <- rename_columns("position")

# Combine the original data frame with the renamed one-hot encoded columns

df_one_hot <- cbind(
  reduced_data,
  one_hot_land_surface_condition,
  one_hot_foundation_type,
  one_hot_roof_type,
  one_hot_ground_floor_type,
  one_hot_other_floor_type,
  one_hot_position
) %>% select(-one_of(c("land_surface_condition", "foundation_type", "roof_type", 
                   "ground_floor_type", "other_floor_type", "position")))

# Convert one-hot encoded columns to integers
df_one_hot[] <- lapply(df_one_hot, as.integer)

head(df_one_hot)

```

Finally, we are going to store the processed data in a new variable, in order to use it in the next steps.

```{r}
processed_data <- df_one_hot
```

## Principal Component Analysis (PCA)

PCA is a dimensionality reduction technique that transforms the data into a lower-dimensional space. This technique is useful for reducing the number of features in the dataset while retaining the most important information. In this section, we will perform PCA on the dataset to reduce the number of features.

First of all, we will take a look at the data we have obtained after the previous steps.

```{r}
head(processed_data)
dim(processed_data)
str(processed_data)
summary(processed_data)
```

Before applying PCA, we need to prepare the data by doing the following:

-   Remove the target variable (`damage_grade`) from the dataset. We will add it back after applying PCA.
-   Standardize the data as PCA is sensitive to the scale of the variables. We will use the `scale` function to standardize the numerical variables so that they have a mean of 0 and a standard deviation of 1.

```{r}
# Remove the target variable
data_without_target <- processed_data %>% select(-damage_grade)

# Standardize the data
data_scaled <- scale(data_without_target)
```

Now, we can apply PCA to the standardized data. We will use the `prcomp` function to perform PCA and extract the principal components. We can also view the PCA results, including the rotation matrix, which shows the correlation between the original variables and the principal components.

```{r}
pca_result <- prcomp(data_scaled, center = TRUE, scale. = TRUE)

# View the PCA results
summary(pca_result)

# Get the rotation matrix
pca_result$rotation
```

Now, we must decide how many principal components to keep. There are several methods to determine the number of components to retain, such as the Kaiser criterion, scree plot, and cumulative explained variance. In this case, we will use the Kaiser criterion, which suggests retaining components with eigenvalues greater than 1.

```{r}
# Calculate the eigenvalues
eigenvalues <- pca_result$sdev^2

# View the eigenvalues
eigenvalues

# Select the components with eigenvalues greater than 1
num_components <- sum(eigenvalues > 1)

# Number of components to keep
num_components
```

According to the Kaiser criterion, we should keep the first 6 principal components. We can extract these components from the PCA results and add the target variable back to the dataset after applying PCA.

```{r}
# Convert the principal components to a data frame
pca_components_df <- as.data.frame(pca_result$x)

# Extract the first 6 principal components
pca_components <- pca_components_df[, 1:num_components]

# Add the target variable back to the dataset
pca_data <- cbind(pca_components, processed_data$damage_grade)

head(pca_data)
dim(pca_data)
str(pca_data)
summary(pca_data)
```

As a result, we have obtained a dataset with 6 principal components and the target variable `damage_grade` after applying PCA using the Kaiser criterion. This suposes a significant reduction in the number of features. We can use this dataset for further analysis and modeling in the next steps.

## Imbalanced data handling

In this section, we will address the issue of imbalanced data with SMOTE (Synthetic Minority Over Sampling Technique). SMOTE is a well-known algorithm to fight this problem. The general idea of this method is to artificially generate new examples of the minority class using the nearest neighbors of these cases. Furthermore, the majority class examples are also under-sampled, leading to a more balanced dataset.

First, we will check the distribution of the target variable `damage_grade` to confirm the imbalance in the dataset. Then, we will apply SMOTE to balance the data.

```{r}
# Check the distribution of the target variable
table(processed_data$damage_grade)
dim(processed_data)

```

As we can see, the dataset is imbalanced, with a higher number of buildings in damage grade 2 compared to the other grades. We will use the `performanceEstimation` package to apply SMOTE to balance the data.

```{r}

# library(performanceEstimation)
# balanced_data <- smote(damage_grade ~ ., data = processed_data)
# 
# # Check the distribution of the target variable in the balanced data
# table(balanced_data$damage_grade)
# dim(balanced_data)

```